{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T20:09:36.824740Z",
     "start_time": "2020-05-04T20:09:36.816919Z"
    }
   },
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    <strong>Exercise Session 2 - Computer vision: basic image processing and feature tracking<br/> Duration : 4 hours (2 in session + 2 at home)</strong><br/><br/>\n",
    "    <span style=\"text-decoration:underline;font-weight:bold;\">How to use this notebook?</span><br/>\n",
    "    This notebook is made of text cells and code cells. The code cells have to be <strong>executed</strong> to see the result of the program. To execute a cell, simply select it and click on the \"play\" button (<span style=\"font: bold 12px/30px Arial, serif;\">&#9658;</span>) in the tool bar just above the notebook, or type <code>shift + enter</code>. It is important to execute the code cells in their order of appearance in the notebook.<br/>\n",
    "You can make use of the table of contents to navigate easily between sections.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"justify;text-justify: inter-word\">\n",
    "So that you may familiarise with the notebooks and the basic python syntax, the exercises are provided in notebook form and whenever there are any calculations to be made, we encourage you to do them by code. Also, if you want to take notes, we encourage you to use the markdown or Raw NBConvert cells. \n",
    "</div>\n",
    "\n",
    "Note : the images used in this exercise session were taken from tutorials online. And although the references are not provided here so that you don't have a sneak peak at the solution, you will not be excused if you do not cite your sources in the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Goals\" data-toc-modified-id=\"Learning-Goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Learning Goals</a></span></li><li><span><a href=\"#Activity-1---Exploring-Different-Image-Processing-Filters\" data-toc-modified-id=\"Activity-1---Exploring-Different-Image-Processing-Filters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Activity 1 - Exploring Different Image Processing Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Denoising-Filters\" data-toc-modified-id=\"Denoising-Filters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Denoising Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-OpenCV,-Reading-and-Displaying-an-Image\" data-toc-modified-id=\"Importing-OpenCV,-Reading-and-Displaying-an-Image-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Importing OpenCV, Reading and Displaying an Image</a></span></li><li><span><a href=\"#Testing-the-output-of-the-different-filters-on-the-image-to-understand-how-they-work\" data-toc-modified-id=\"Testing-the-output-of-the-different-filters-on-the-image-to-understand-how-they-work-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Testing the output of the different filters on the image to understand how they work</a></span></li></ul></li><li><span><a href=\"#Edge-Detection-Filters\" data-toc-modified-id=\"Edge-Detection-Filters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Edge Detection Filters</a></span></li></ul></li><li><span><a href=\"#Activity-2---Image-Tracking\" data-toc-modified-id=\"Activity-2---Image-Tracking-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Activity 2 - Image Tracking</a></span></li><li><span><a href=\"#Activity-3---Implementing-the-Hough-Transform-to-Identify-Straight-Lines-in-an-Image\" data-toc-modified-id=\"Activity-3---Implementing-the-Hough-Transform-to-Identify-Straight-Lines-in-an-Image-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Activity 3 - Implementing the Hough Transform to Identify Straight Lines in an Image</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-the-libraries\" data-toc-modified-id=\"Importing-the-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Importing the libraries</a></span></li><li><span><a href=\"#Loading-and-displaying-the-image\" data-toc-modified-id=\"Loading-and-displaying-the-image-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Loading and displaying the image</a></span></li><li><span><a href=\"#Creating-the-hough-transform-matrix\" data-toc-modified-id=\"Creating-the-hough-transform-matrix-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Creating the hough transform matrix</a></span></li><li><span><a href=\"#Identifying-the-maxima-of-the-Hough-Transform\" data-toc-modified-id=\"Identifying-the-maxima-of-the-Hough-Transform-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Identifying the maxima of the Hough Transform</a></span><ul class=\"toc-item\"><li><span><a href=\"#Isolating-the-brightest-spots-through-thresholding\" data-toc-modified-id=\"Isolating-the-brightest-spots-through-thresholding-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Isolating the brightest spots through thresholding</a></span></li><li><span><a href=\"#Enlarging-the-size-of-the-spots-through-morphological-operators\" data-toc-modified-id=\"Enlarging-the-size-of-the-spots-through-morphological-operators-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Enlarging the size of the spots through morphological operators</a></span></li><li><span><a href=\"#Identifying-the-Centroids'-Locations-Through-Contours\" data-toc-modified-id=\"Identifying-the-Centroids'-Locations-Through-Contours-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Identifying the Centroids' Locations Through Contours</a></span></li></ul></li><li><span><a href=\"#Computing-the-location-of-the-straight-lines-in-the-original-image-and-drawing-them\" data-toc-modified-id=\"Computing-the-location-of-the-straight-lines-in-the-original-image-and-drawing-them-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Computing the location of the straight lines in the original image and drawing them</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Learning Goals\n",
    "\n",
    "\n",
    "- Understanding the main steps of computer vision, from cameraframes to high-level information\n",
    "\n",
    "\n",
    "- Implementing the tracking of an object. \n",
    "\n",
    "\n",
    "- Implementing the hough transform of an image to identify segments\n",
    "\n",
    "\n",
    "Note that you will need opencv for this exercise session\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:16.519011Z",
     "start_time": "2021-09-23T13:14:14.397804Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python tqdm matplotlib numpy ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!pip install --upgrade tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 - Exploring Different Image Processing Filters\n",
    "\n",
    "The first step of many image processing pipelines includes image filtering. \n",
    "\n",
    "\n",
    "You know that filters are applied through convolution and that this can be done in the Fourier space. However, they can also be applied in the image space by computing a moving weighted average between the kernel corresponding to the given filter and the original image. This is illustrated in the image below (taken from [here](https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37)). \n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/kernel_conv.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "However, rather than go into the detail of the different implementations of convolutions and the kernels associated to the different filters (which do not get me wrong, you do have to know for the exam), the objective here is to take time to learn about the differences between the different filers by using a computer vision library (opencv) to test them on a dataset of images. \n",
    "\n",
    "\n",
    "The types of filters we are intersted (for the moment at least) are :\n",
    "\n",
    "- **Denoising filters (low pass filters)**, which help reduce the amount of noise in an image and smoothen it out. Examples of denoising filters include average filtering, median filtering, gaussian filtering. \n",
    "\n",
    "- **Edge detection filters (high pass filters)**. In the course you have seen the sobel, robert and prewitt filters as well as the Canny Edge filter. \n",
    "\n",
    "\n",
    "If you are interested in having more details about these filtering approaches you can have a look at the introduciton to image filtering provided [here](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html).\n",
    "\n",
    "\n",
    "## Denoising Filters\n",
    "\n",
    "The goal here is to undestand the different denoising filters that opencv has to offer and when to use them: \n",
    "\n",
    "- **average filtering** with the cv2.blur function\n",
    "\n",
    "\n",
    "- **median filtering** with the cv2.medianBlur function\n",
    "\n",
    "\n",
    "- **gaussian filtering** with the cv2.GaussianBlur function\n",
    "\n",
    "\n",
    "- **bilateral filtering** with the cv2.bilateralFilter function\n",
    "\n",
    "\n",
    "You can call the help function for each of these functions to access the documentation. In the cell below is an example of how to use the help function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:16.528796Z",
     "start_time": "2021-09-23T13:14:16.522254Z"
    }
   },
   "outputs": [],
   "source": [
    "help(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing OpenCV, Reading and Displaying an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by importing opencv for the image processing and matplotlib for the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.069909Z",
     "start_time": "2021-09-23T13:14:16.532721Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load one of the images and display it. \n",
    "\n",
    "Note that opencv loads images in BGR mode whilst matplotlib (and most systems in general) work in RGB. That is why when we display the image we need to inverse the order of the last dimension\n",
    "\n",
    "Here we have loaded an image that contains Gaussian noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.438028Z",
     "start_time": "2021-09-23T13:14:20.072328Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'images/download.png'\n",
    "img = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the output of the different filters on the image to understand how they work\n",
    "\n",
    "Apply the filters on the original image and display the output. Play with the parameters to see how they influence the output to get a sense of how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:37.436589Z",
     "start_time": "2020-05-05T15:39:35.560702Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the particularity of the bilateral filter?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now apply the filters on the `gaussian_noise` image. \n",
    "\n",
    "![Pentagon](images/gaussian_noise.png)\n",
    "\n",
    "***Based on your observations, which filter is most adapted if you want to remove gaussian type noise?***\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:39.487965Z",
     "start_time": "2020-05-05T15:39:37.438485Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Which filter would you use if you wanted to remove salt and pepper type noise like the one in the image below?***\n",
    "    \n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/salt_pepper_noise.png)\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection Filters\n",
    "\n",
    "Let's focus on three filters:\n",
    "\n",
    "- Sobel (`cv2.Sobel` function): this filter relies on detecting edges in one direction than in the perpendicular to isolate all edges in the image. As it is very close in the underlying principle to the Robers and Prewitt filters we just focus on this one. \n",
    "\n",
    "- Laplacian (`cv2.Laplacian` function): this is a new filter which you did not see in the course. Intead of looking for the maximum of the derivative of an image as the Sobel filter does, this filter looks for the zero crossing of the second derivative. \n",
    "\n",
    "- Canny (`cv2.Canny` function): a bit more elaborate than the previous two, the gradient of the image is computed but there is an extra processing that looks to validate whether a pixel is actually part of an edge or not. If you are interested in the details you can have a look [here](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html).\n",
    "\n",
    "\n",
    "\n",
    "Apply the different edge detection filters on the ``SanFrancisco`` image and try to get the best results you can by tuning the parameters of the different algorithms.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](images/SanFrancisco.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "***Generally, what should you do before applying the edge detection to improve the results of the filtering?***\n",
    "\n",
    "\n",
    "In particular, for the Sobel filter try to answer the following questions: \n",
    "\n",
    "***When the derivative is along the horizontal axis, what is the effect on the vertical axis?***\n",
    "\n",
    "\n",
    "***How do you implement a Sobel filter that works in both directions?***\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the image in grayscale to be able to apply the edge detection filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:20.643718Z",
     "start_time": "2021-09-23T13:14:20.439969Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('images/SanFrancisco.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can work on the rest :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:42.566291Z",
     "start_time": "2020-05-05T15:39:42.302449Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2 - Image Tracking\n",
    "\n",
    "You can track the position of an object in an image by using what is called template matching. Let's apply template matching on a video of the Thymio robot following a line and attempt to track the position of the robot throughout the video using the different techniques that opencv provides. \n",
    "\n",
    "First we are going to load the video and store the different frames. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.454140Z",
     "start_time": "2021-09-23T13:14:20.645549Z"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('images/thymio_line_following.mp4')\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "video_imgs = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        video_imgs.append(frame)\n",
    "    # Break the loop\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(\"There are {} frames in the video\".format(len(video_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one of the frames and crop the Thymio to make our template. In frame 420, the Thymio is located between x = \\[446, 611\\] and y = \\[312, 484\\] (determined using the cursor on the image). Why are we doing it this way? Template matching does not work well if you take a reference / template image that is not to scale w.r.t the image you are going to be looking in. One solution is to use multi-scale [template matching](https://www.pyimagesearch.com/2015/01/26/multi-scale-template-matching-using-python-opencv/). We are not going to go into that here but the link provided gives an example of how that could be implemented.  \n",
    "\n",
    "Let's display the portion of frame 420 where the Thymio is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.659420Z",
     "start_time": "2021-09-23T13:14:22.456052Z"
    }
   },
   "outputs": [],
   "source": [
    "template = video_imgs[420][312:484,446:611,:]\n",
    "plt.figure()\n",
    "plt.imshow(template)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the opencv tutorial [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html) and apply the different template matching techniques on the image and the template. \n",
    "\n",
    "***Which methods seems the most robust and why?***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:39:46.408722Z",
     "start_time": "2020-05-05T15:39:45.202368Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have selected the algorithm that you believe to be the most appropriate, apply it to the set of images that were extracted from the video. To visualise the result of the process, we have provided a browse images function which when provided the individual images lets you nagivate through them with a slider. \n",
    "\n",
    "***What are the limitations of template matching?***\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:14:22.708767Z",
     "start_time": "2021-09-23T13:14:22.662504Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def browse_images(images, titles = None):\n",
    "    if titles == None:\n",
    "        titles = [i for i in range(len(images))]\n",
    "        \n",
    "    n = len(images)\n",
    "    def view_image(i):\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title(titles[i], y=-0.5)\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:17.624471Z",
     "start_time": "2020-05-05T15:39:46.410795Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\ If you have problems with the slider widget, make sure you have installed and activated the jupyter-js-widgets/extension (see https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3 - Implementing the Hough Transform to Identify Straight Lines in an Image\n",
    "\n",
    "The goal of this exercise is going to be to code the Hough transform to identify straight lines in an image. We are going to use the following image throughout the exercise in order to test the different steps. The image is located in the Images folder under ``pentagon.png``\n",
    "\n",
    "As this is one of your first \"big\" coding exercises we are going to guide you through it. \n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](Images/pentagon.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Importing the libraries \n",
    "\n",
    "We are going to need the :\n",
    "\n",
    "- `opencv` for image processing\n",
    "\n",
    "- `math` for the trigonometry functions\n",
    "\n",
    "- `numpy` for array processing\n",
    "\n",
    "- `matplotlib` for plotting\n",
    "\n",
    "> Note that the exercise was inspired from the example [here](https://www.science-emergence.com/Articles/Implementing-a-simple-python-code-to-detect-straight-lines-using-Hough-transform/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:02:38.201766Z",
     "start_time": "2020-09-22T14:02:38.198782Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and displaying the image\n",
    "\n",
    "Start by giving the filename with the full path so that the mpimg module can load the image with the imread function. \n",
    "\n",
    "Then display the image object with plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:02:38.432427Z",
     "start_time": "2020-09-22T14:02:38.205044Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'images/pentagon.png'\n",
    "img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the hough transform matrix\n",
    "\n",
    "1. We start by creating a numpy array of arbitrary dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:02:38.436611Z",
     "start_time": "2020-09-22T14:02:38.434168Z"
    }
   },
   "outputs": [],
   "source": [
    "# We choose to have a final matrix of the following\n",
    "# dimensions. This is an arbitrary decision\n",
    "\n",
    "r_dim = 200 \n",
    "theta_dim = 300\n",
    "\n",
    "# Now, create the hough transform matrix (filled with zeros), called hough_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we need to relate the dimensions of this new matrix to the corresponding values along the rho and theta axes. This means we need to determine the minimum and maximum values of theta and rho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:02:38.441259Z",
     "start_time": "2020-09-22T14:02:38.438750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the values of theta min and theta max\n",
    "\n",
    "\n",
    "# From the image shape, determine rho min and rho max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compute the hough transform of the original image. The pseudo code is the following :\n",
    "\n",
    "    for all pixels:\n",
    "        if the pixel is white:\n",
    "              do nothing\n",
    "        else:\n",
    "            for theta values :\n",
    "                compute rho from the pixel coordinates and theta\n",
    "                convert rho to the image space\n",
    "                update the hough space matrix for the theta and rho coordinates in the array\n",
    "                \n",
    "You should get a result similar to this one \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](Images/houghTransform.png)\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the maxima of the Hough Transform\n",
    "\n",
    "After calculating the Hough space, the next step is to find the local extrema. This will give us the rho and theta parameters of the different straight lines in the image. \n",
    "\n",
    "To find the extrema, we will do the following :\n",
    "\n",
    "1. isolate the brightest spots\n",
    "\n",
    "2. enlarge the size of the spots\n",
    "\n",
    "3. find the centroid of the spots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating the brightest spots through thresholding\n",
    "\n",
    "To isolate the bright spots and remove the gradients around the maxima we are going to apply **thresholding** to convert the image into 0's for the dark areas and 1's around the maximas. Have a look [here](https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html) at the different thresholding functions that opencv has to offer. \n",
    "\n",
    "Play with the parameters to try to obtain an image similar to this one \n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/thresholding_output.png)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:20.301138Z",
     "start_time": "2020-05-05T15:40:20.056494Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlarging the size of the spots through morphological operators\n",
    "\n",
    "To identify the location of the local maxima, we will make use of find contours. However the spots in the image are too small to apply the opencv functions. That is why we are going to start by increasing their size through morphological operations. Have a look [here](https://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html) to see the different operators and their effect on a binary image. \n",
    "\n",
    "\n",
    "You should get an image like this one : \n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/morphological_output.png)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:20.547095Z",
     "start_time": "2020-05-05T15:40:20.304135Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:18:47.171768Z",
     "start_time": "2020-05-04T22:18:47.005549Z"
    }
   },
   "source": [
    "### Identifying the Centroids' Locations Through Contours \n",
    "\n",
    "Now we have bigger spots and we are going to use Opencv's built in functions to find the contours and deduce the location of the centroid in the hough space. [An example is provided here](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/). \n",
    "\n",
    "You should get a result similar to this one : \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](Images/centroid_output.png)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:20.790040Z",
     "start_time": "2020-05-05T15:40:20.548963Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the location of the straight lines in the original image and drawing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the maxima obtained, we can now compute the corresponding segments in the original image. You should get a result similar to this one. \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![Pentagon](images/hough_transform.png)\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:40:21.028148Z",
     "start_time": "2020-05-05T15:40:20.792305Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
